{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating problem\n",
    "\n",
    "假设想要找到使J(w,b)最小的w,b值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   #启动tensorflow\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.],[-10.],[25.]])  # 定义一个数组，这是要接入x的数据\n",
    "\n",
    "w = tf.Variable(0,dtype=tf.float32)   # 在TensorFlow中，用tf.Variable来定义参数，将w初始化为0\n",
    "                                      # cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25) #定义损失函数\n",
    "\n",
    "# 如何把训练数据加入tensorflow\n",
    "x = tf.placeholder(tf.float32,[3,1])    # 把x定义为placeholder，类型是float32，称为【3，1】的数组\n",
    "\n",
    "cost = w**2 - 10*w + 25 \n",
    "cost = x[0][0]*w**2+x[1][0]*w+x[2][0] # x控制了cost的系数\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost) # 定义train为学习算法，用GradientDescentOptimizer梯度下降法优化器 使损失函数最小化\n",
    "# 符合表达习惯的\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "# 让TensorFlow评估一个变量\n",
    "print(session.run(w))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# 用一种方法把这个系数数组接入变量x\n",
    "session.run(train,feed_dict={x:coefficients}) #运行一步梯度下降法\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.99999\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000): # 运行梯度下降1000次迭代\n",
    "    session.run(train,feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解释：\n",
    "\n",
    "- w是我们想要优化的参数，因此称它为变量Variable。\n",
    "\n",
    "- tensorflow 可以完成反向传播，我们只需要完成前向传播就可以了，也就是costfunction。因为它已经内置在add multiply和平方函数中。并且也内置了一般的加减运算等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- 便于把训练数据加入损失方程\n",
    "\n",
    "x = tf.placeholder(tf.float32,[3,1])\n",
    "\n",
    "- 当运行训练迭代的时候，用feed_dict来让x等于coefficients\n",
    "\n",
    "session.run(train,feed_dict={x:coefficients})\n",
    "\n",
    "- 如果做mini-batch梯度下降，每次迭代时需要插入不同的mini-batch，\n",
    " 那么每次迭代就用feed_dict来喂入训练集的不同子集\n",
    " \n",
    "session.run(train,feed_dict={x:coefficients})\n",
    "- 符合表达习惯的\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(init)\n",
    "\n",
    "- 可以用这种方法来代替\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "session.run(init)\n",
    "\n",
    "print(session.run(w))\n",
    "\n",
    "- 以上三句话的含义\n",
    "\n",
    "就是让TensorFlow建立计算图，计算图所做的就是取cost。\n",
    "\n",
    "TensorFlow的优点在于，通过用这个计算损失的，计算图基本实现前向传播，并且已经内置的所有必要的反向函数。\n",
    "这也就是为什么通过内置函数来计算前向函数，也能自动实现反向传播。\n",
    "\n",
    "- 如果想用其他的优化方法，替换\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
